{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Web scraping & data preparation - STU59362\n",
    "!pip install selenium\n",
    "\n",
    "# Import libraries - STU59362\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "# Driver setup for opening URl in Google Chrome \n",
    "!pip install webdriver-manager\n",
    "\n",
    "# Import Driver\n",
    "from selenium import webdriver\n",
    "PATH = 'chromedriver.exe'\n",
    "browser = webdriver.Chrome(executable_path = PATH)\n",
    "\n",
    "# Specify Search URL - STU59362\n",
    "URL = 'https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans'\n",
    "browser.get(URL)\n",
    "\n",
    "# STU59362 = To generate a blank data frame to store usernames, dates and postings of 100 users.\n",
    "Forum_Discussions = pd.DataFrame(columns = ['User_Name', 'Date','Forum_Discussions'])\n",
    "\n",
    "y = 1\n",
    "while (y<=435): # Total number of web pages to be crawled is 435\n",
    "    # Running while loop only till we get 500 comments while scrolling to the bottom of the website\n",
    "    if (len(Forum_Discussions)<500):\n",
    "        url = 'https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p' + str(y)\n",
    "        browser.get(url)\n",
    "        # Finding elements\n",
    "        usernames = browser.find_elements_by_xpath(\"//*[contains(@id,'Comment_')]\")\n",
    "        message_usernames = []\n",
    "        for x in usernames:\n",
    "            message_usernames.append(x.get_attribute('id'))\n",
    "\n",
    "        for j in message_usernames:\n",
    "            \n",
    "            #Fetching the user names of each user on a page\n",
    "            username_element = browser.find_elements_by_xpath('//*[@id=\"' + j +'\"]/div/div[2]/div[1]/span[1]/a[2]')[0]\n",
    "            username = username_element.text\n",
    "                        \n",
    "            #Fetching the date of each user on a page\n",
    "            posting_date = browser.find_elements_by_xpath('//*[@id=\"' + j +'\"]/div/div[2]/div[2]/span[1]/a/time')[0]\n",
    "            date = posting_date.get_attribute('title')\n",
    "          \n",
    "            #Fetching the forum postings of each user on a page\n",
    "            user_post = browser.find_elements_by_xpath('//*[@id=\"' + j +'\"]/div/div[3]/div/div[1]')[0]\n",
    "            message = user_post.text\n",
    "            \n",
    "            # To extract any present Block Quote\n",
    "            block_quote = browser.find_element_by_xpath('//*[@id=\"' + j + '\"]/div/div[3]/div/div[1]')\n",
    "            block_quote_class = block_quote.find_elements_by_class_name('UserQuote')\n",
    "            block_text = ''\n",
    "            if len(block_quote_class)>0:\n",
    "                block_text = block_quote_class[0].text\n",
    "            \n",
    "            # To replace block quotes\n",
    "            message = message.replace(block_text,\"\")\n",
    "            \n",
    "           #Adding userid, date, and comment for each user posting in the dataframe \n",
    "        \n",
    "            Forum_Discussions.loc[len(Forum_Discussions)] = [username,date,message]\n",
    "            #print(username,date,message)\n",
    "        y=y+1\n",
    " \n",
    "    else:\n",
    "        break\n",
    "print(Forum_Discussions)\n",
    "\n",
    "print (Forum_Discussions.head())\n",
    "\n",
    "\n",
    "# import necessary libraries\n",
    "from time import sleep\n",
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Making a copy of the original dataframe and converting the data into a standardised csv file - STU59362\n",
    "import copy\n",
    "Forum_Discussions_copy = copy.deepcopy(Forum_Discussions)\n",
    "\n",
    "def remove_space(s):\n",
    "    return s.replace(\"\\n\",\" \")\n",
    "\n",
    "Forum_Discussions_copy['Forum_Discussions'] = Forum_Discussions_copy['Forum_Discussions'].apply(remove_space)\n",
    "Forum_Discussions_copy.to_csv('postings.csv', header=True, sep=',') \n",
    "\n",
    "\n",
    "# Cleaning the data\n",
    "# A \"veh_models\" csv (UTF-8 encoded) file had been created and saved in the working directory.\n",
    "# It contains the brands and models of vehicles \n",
    "\n",
    "Forum_Discussions_copy = pd.read_csv('postings.csv') #reading postings\n",
    "#Reading vehicle models\n",
    "veh_models = pd.read_csv(\"veh_models.csv\", header = None, names = ['brand','model'], encoding='windows-1252')\n",
    "Forum_Discussions_copy = Forum_Discussions_copy.dropna()\n",
    "Forum_Discussions_copy.reset_index(inplace  = True)\n",
    "\n",
    "# Cleaning process \"functions\"\n",
    "\n",
    "def removepunc(item):\n",
    "    for p in punctuation:\n",
    "        item = item.lstrip().replace(p,'')\n",
    "    return item\n",
    "\n",
    "def lowerize(x):\n",
    "    return x.lower()\n",
    "\n",
    "Forum_Discussions_copy['Forum_Discussions_clean'] = Forum_Discussions_copy['Forum_Discussions'].apply(removepunc).apply(lowerize)\n",
    "veh_models['brand'] = veh_models['brand'].apply(removepunc)\n",
    "\n",
    "\n",
    "\n",
    "def vehmodel_to_brand(s):\n",
    "    for k in veh_models.index.values:\n",
    "        s = s.replace(veh_models[\"model\"][k].lower(),veh_models[\"brand\"][k].lower())\n",
    "    return s\n",
    "Forum_Discussions_copy['Forum_Discussions_vehmodel_replace'] = Forum_Discussions_copy['Forum_Discussions_clean'].apply(vehmodel_to_brand)\n",
    "\n",
    "\n",
    "Forum_Discussions_copy['Forum_Discussions_appear'] = Forum_Discussions_copy['Forum_Discussions_vehmodel_replace'].apply(word_tokenize).apply(set).apply(list)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(s):\n",
    "    return [w for w in s if not w in stop_words] \n",
    "    \n",
    "Forum_Discussions_copy['final_Forum_Discussions'] =  Forum_Discussions_copy['Forum_Discussions_appear'].apply(remove_stopwords)\n",
    "# filtered_sentence = [w for w in temp['Forum_Discussions_appear'] if not w in stop_words]\n",
    "\n",
    "\n",
    "count = []\n",
    "for j in range(len(Forum_Discussions_copy)):\n",
    "    count+=Forum_Discussions_copy['final_Forum_Discussions'][j]\n",
    "    \n",
    "\n",
    "from nltk import FreqDist\n",
    "freq_dist = nltk.FreqDist(count)\n",
    "\n",
    "\n",
    "\n",
    "unique_veh_models = veh_models['brand'].drop_duplicates().tolist()\n",
    "\n",
    "\n",
    "# Frequency Distribution of 600 leading brands\n",
    "leading_words = freq_dist.most_common(600)\n",
    "leading_brands = []\n",
    "for (key, items) in leading_words:\n",
    "    if key in unique_veh_models:\n",
    "        model_sum_total = (key,items)\n",
    "        leading_brands.append(model_sum_total)\n",
    "        \n",
    "        \n",
    "# Leading 20 vehicle brands\n",
    "leading_20_brands_sum_total = leading_brands[:20]\n",
    "print ('The leading 20 vehicle brands together with their frequency distributions are:\\n' , leading_20_brands_sum_total[:20])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (veh_models)\n",
    "\n",
    "#Calculating lift ratios for association between 10 leading brands\n",
    "# Fetching the top 10 brands.\n",
    "leading_10_brands =[]\n",
    "for brand, count in leading_10_brands_counts:\n",
    "    leading_10_brands.append(brand)\n",
    "\n",
    "new_df = pd.DataFrame(columns = leading_10_brands)\n",
    "\n",
    "def brand_mentioned(item):\n",
    "    if brand in item:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "      \n",
    "for brand in leading_10_brands:\n",
    "    new_df[brand] = Forum_Discussions_copy['final_Forum_Discussions'].apply(brand_mentioned)\n",
    "    \n",
    "# Calculating lift among top brands\n",
    "df2=pd.DataFrame(columns = leading_10_brands)\n",
    "for i in range(len(leading_10_brands)):\n",
    "    new_list = []\n",
    "    for j in range(len(leading_10_brands)):\n",
    "        if (i!=j):\n",
    "            numerator = ((new_df[leading_10_brands[i]] + new_df[leading_10_brands[j]]) > 1).sum()\n",
    "            denominator = new_df[leading_10_brands[j]].sum()*new_df[leading_10_brands[i]].sum()\n",
    "            lift = numerator*len(new_df)/denominator\n",
    "            df2.loc[leading_10_brands[i],leading_10_brands[j]] = lift\n",
    "print ('Below are the lift ratios among leading brands\\n')\n",
    "df2\n",
    "\n",
    "# Leading 10 vehicle brands\n",
    "leading_10_brands_sum_total = leading_brands[:10]\n",
    "print ('The leading 10 vehicle brands together with their frequency distributions are:\\n' , leading_10_brands_sum_total[:10])\n",
    "\n",
    "\n",
    "# Calculating lift ratios for association between 10 leading brands\n",
    "\n",
    "# Fetching the leading 10 brands.\n",
    "\n",
    "leading_10_brands =[]\n",
    "for brand, count in leading_10_brands_sum_total:\n",
    "    leading_10_brands.append(brand)\n",
    "\n",
    "new_df = pd.DataFrame(columns = leading_10_brands)\n",
    "\n",
    "def brand_mentioned(item):\n",
    "    if brand in item:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "      \n",
    "for brand in leading_10_brands:\n",
    "    new_df[brand] = Forum_Discussions_copy['final_Forum_Discussions'].apply(brand_mentioned)\n",
    "    \n",
    "# Lift ratio calculation among leading brands\n",
    "\n",
    "df2=pd.DataFrame(columns = leading_10_brands)\n",
    "for x in range(len(leading_10_brands)):\n",
    "    new_list = []\n",
    "    for y in range(len(leading_10_brands)):\n",
    "        if (x!=y):\n",
    "            numerator = ((new_df[leading_10_brands[x]] + new_df[leading_10_brands[y]]) > 1).sum()\n",
    "            denominator = new_df[leading_10_brands[y]].sum()*new_df[leading_10_brands[x]].sum()\n",
    "            lift = numerator*len(new_df)/denominator\n",
    "            df2.loc[leading_10_brands[x],leading_10_brands[y]] = lift\n",
    "print ('The lift ratios among leading brands are:\\n')\n",
    "df2\n",
    "\n",
    "# Calculating the dissimilarity matrix which is the input for plotting MDS plot\n",
    "dissimilarity_matrix = 1/df2\n",
    "np.fill_diagonal(dissimilarity_matrix.values, 0)\n",
    "\n",
    "# Plotting MDS plot\n",
    "from sklearn import manifold\n",
    "seed = np.random.RandomState(seed=3)\n",
    "mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, \n",
    "      random_state=seed, dissimilarity=\"precomputed\", n_jobs=1)\n",
    "results = mds.fit(dissimilarity_matrix)\n",
    "coords = results.embedding_\n",
    "\n",
    "plt.subplots_adjust(bottom = 0.1)\n",
    "plt.scatter(\n",
    "    coords[:, 0], coords[:, 1], marker = 'o'\n",
    "    )\n",
    "for label, x, y in zip(leading_10_brands, coords[:, 0], coords[:, 1]):\n",
    "    \n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy = (x, y), xytext = (-30, 30),\n",
    "        textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'blue', alpha = 0.5),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(df2)\n",
    "\n",
    "#Checking for null values\n",
    "print(Forum_Discussions.isnull().sum())\n",
    "\n",
    "# Most frequent features of cars mentioned in the forum discussions\n",
    "\n",
    "features = pd.read_csv('features.csv') # Read the contents of the file\n",
    "\n",
    "def word_to_features(z):\n",
    "    z = \" \".join(str(y) for y in z)\n",
    "    for k in features.index.values:\n",
    "        z = z.replace(features[\"Feature\"][k].lower(),features[\"Mapping\"][k].lower())\n",
    "    return z\n",
    "Forum_Discussions_copy['Forum_Discussions_features_replace'] = Forum_Discussions_copy['final_Forum_Discussions'].apply(word_to_features)\n",
    "\n",
    "\n",
    "count = []\n",
    "for k in range(len(Forum_Discussions_copy)):\n",
    "    count+=Forum_Discussions_copy['Forum_Discussions_features_replace'][k]\n",
    "feat_freq = nltk.FreqDist(count)\n",
    "features_unique = features['Mapping'].drop_duplicates().tolist() # The unique mapping features\n",
    "\n",
    "leading_words = freq_dist.most_common(2000) # Frequecy distibution of the most 2000 common words\n",
    "leading_features = []\n",
    "for (key, items) in leading_words:\n",
    "    if key in features_unique:\n",
    "        feature_counts = (key,items)\n",
    "        leading_features.append(feature_counts) \n",
    "        \n",
    "\n",
    "leading_5_features_counts = leading_features[:5]\n",
    "\n",
    "# The dataframe \n",
    "print('The shape of the data is \\n', features, '\\n')\n",
    "\n",
    "# The unique mapping features\n",
    "print('The unique mapping features are:\\n', features_unique, '\\n')\n",
    "\n",
    "print ('The frequency distribution of the leading 5 features are:\\n' , leading_5_features_counts)\n",
    "\n",
    "# Most strongly associated of the features among the leading 5 brands\n",
    "\n",
    "# Fetching leading 5 features\n",
    "leading_5_features =[]\n",
    "for feature, count in leading_5_features_counts:\n",
    "    leading_5_features.append(feature)\n",
    "\n",
    "features_df = pd.DataFrame(columns = leading_5_features)\n",
    "\n",
    "def feature_mentioned(item):\n",
    "    if feature in item:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "      \n",
    "for feature in leading_5_features:\n",
    "      \n",
    "    features_df[feature] = Forum_Discussions_copy['Forum_Discussions_features_replace'].apply(feature_mentioned)    \n",
    "\n",
    "\n",
    "# Calculating Lift between leading 5 brands and leading 5 features\n",
    "df3=pd.DataFrame(columns = leading_5_features)\n",
    "leading_5_brands = leading_10_brands[:5]\n",
    "for i in range(len(leading_5_brands)):\n",
    "    new_list = []\n",
    "    for j in range(len(leading_5_features)):\n",
    "        numerator = ((new_df[leading_5_brands[i]] + features_df[leading_5_features[j]]) > 1).sum()\n",
    "        denominator = new_df[leading_5_brands[i]].sum()*features_df[leading_5_features[j]].sum()\n",
    "        lift_brand_features = numerator*len(features_df)/denominator\n",
    "        df3.loc[leading_5_brands[i],leading_5_features[j]] = lift_brand_features\n",
    "\n",
    "print ('The lift ratios between leading 5 brands and leading 5 features are: \\n\\n', df3)\n",
    "\n",
    "print('\\n', end=\"\")\n",
    "\n",
    "#Display the line graph chart of the lift ration\n",
    "df3.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print ('The lift ratios between leading 5 brands and leading 5 features are: \\n')\n",
    "df3\n",
    "\n",
    "\n",
    "# The most desired target car to own or likely to purchse\n",
    "\n",
    "desires = pd.read_csv(\"desires.csv\") # Read the dataframe\n",
    "\n",
    "# Common target ambitions mentioned in the forum discussions replaced by \"desires\"\n",
    "\n",
    "def ambition(s):\n",
    "    #s = \" \".join(str(x) for x in s)\n",
    "    for i in desires['word'].index.values:\n",
    "        s = s.replace(desires['word'][i],desires['target'][i])\n",
    "    return s\n",
    "Forum_Discussions_copy['Forum_Discussions_amb_replace'] = Forum_Discussions_copy['Forum_Discussions_features_replace'].apply(ambition)\n",
    "\n",
    "\n",
    "ambition_df = pd.DataFrame(columns = ['desires'])\n",
    "\n",
    "def ambition_mentioned(item):\n",
    "    if amb in item:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for amb in desires['target'].unique():\n",
    "    ambition_df[amb] = Forum_Discussions_copy['Forum_Discussions_amb_replace'].apply(ambition_mentioned)\n",
    "\n",
    "    \n",
    "    # Calculating Lift ratios between leading 5 brands and desires\n",
    "    ambition_df2=pd.DataFrame(columns = ['desires'])\n",
    "leading_5_brands = leading_10_brands[:5]\n",
    "for i in range(len(leading_5_brands)):\n",
    "    new_list = []\n",
    "    for j in range(len(desires['target'].unique())):\n",
    "        numerator = ((new_df[leading_5_brands[i]] + ambition_df['desires']) > 1).sum()\n",
    "        denominator = new_df[leading_5_brands[i]].sum()*ambition_df['desires'].sum()\n",
    "        lift_brand_target = numerator*len(ambition_df)/denominator\n",
    "        ambition_df2.loc[leading_5_brands[i],'desires'] = lift_brand_target\n",
    "\n",
    "print ('The lift ratios between leading 5 brands and desires are: \\n\\n', ambition_df2)\n",
    "\n",
    "print('\\n', end=\"\")\n",
    "\n",
    "#Display the line graph chart of the lift ratio\n",
    "\n",
    "ambition_df2.plot()\n",
    "plt.show()\n",
    "\n",
    "ambition_df2\n",
    "\n",
    "desires.head()\n",
    "\n",
    "print(features)\n",
    "\n",
    "# The total number of common words in desire.csv file\n",
    "desires.select_dtypes('object').nunique()\n",
    "\n",
    "# The actual words changed to desires\n",
    "print (desires['word'].unique())\n",
    "\n",
    "# The \"desires.csv\" file\n",
    "print (desires)\n",
    "\n",
    "desires['word'].value_counts()\n",
    "\n",
    "# Exporting the final cleaned data frame\n",
    "\n",
    "final_df = Forum_Discussions_copy[['User_Name', 'Date','Forum_Discussions','Forum_Discussions_amb_replace']]\n",
    "final_df.columns = ['User_Name', 'Date','Forum_Discussions','cleaned_Forum_Discussions']\n",
    "final_df.to_csv('final_data_file.csv', header=True, sep=',') \n",
    "\n",
    "# Reading the \"final_data_file\"\n",
    "\n",
    "final_data_file = pd.read_csv('final_data_file.csv')\n",
    "\n",
    "#Displaying the final cleaned data file\n",
    "print(final_data_file)\n",
    "\n",
    "# Displaying final data file\n",
    "final_data_file\n",
    "\n",
    "#To view the full content of the cells of a dataframe\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "print(final_data_file)\n",
    "\n",
    "print(final_data_file.cleaned_Forum_Discussions)\n",
    "\n",
    "print(Forum_Discussions)\n",
    "\n",
    "print (final_df)\n",
    "\n",
    "final_data_file\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
